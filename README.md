# CS205-Project



text
# Knowledge Distillation: ViT â†’ CNN with Dynamic Attention Weighting  
**GitHub-Compatible README**  

---

## ðŸ“Œ Project Overview  
**Objective**: Distill knowledge from a Vision Transformer (ViT) teacher model to a CNN student using **dynamic attention weighting**, prioritizing informative layers via entropy-based scoring.  

**Key Innovation**:  
- **Dynamic Layer Weighting**: Automatically assigns higher importance to teacher layers with lower attention entropy (more decisive spatial focus).  
- **Multi-Stage Transfer**: Combines class probability distillation (KL divergence) with spatial attention alignment (MSE).  

---

## ðŸ›  Setup  

### Requirements  
Core libraries
pip install torch torchvision timm matplotlib

text

### Dataset  
- **CIFAR-10**: Automatically downloaded via PyTorch's `torchvision.datasets`.  
- **Preprocessing**: Images resized to 224x224 for ViT compatibility.  

---

## ðŸ“… Weekly Implementation Plan  

### Day 1: Environment & Data Preparation  
**Tasks**:  
1. Install dependencies and set up data directories.  
2. Implement dataset loading with ViT-compatible transforms.  

**Expected Outcome**:  
- Train/test splits of CIFAR-10 at 224x224 resolution.  

---

### Day 2: Teacher Model Configuration  
**Tasks**:  
1. Load pre-trained ViT-Tiny (`timm` library).  
2. Modify ViT to output attention maps from all layers.  

**Model Specs**:  
| Parameter       | Value     |  
|-----------------|-----------|  
| Architecture    | ViT-Tiny  |  
| Pretraining     | ImageNet-21k |  
| Target Accuracy | ~82%      |  

---

### Day 3: Student Model Design  
**Tasks**:  
1. Build a lightweight CNN with 3 convolutional stages.  
2. Add attention projection layer to match ViT's token dimensions.  

**Student Architecture**:  
| Component       | Channels  | Output Resolution |  
|-----------------|-----------|-------------------|  
| Conv Stage 1    | 64        | 112x112           |  
| Conv Stage 2    | 128       | 56x56             |  
| Conv Stage 3    | 256       | 56x56             |  

---

### Day 4: Dynamic Attention Distillation  
**Key Components**:  
1. **Entropy-Based Weighting**: Prioritizes teacher layers with low entropy (high confidence).  
2. **Loss Composition**:  
   - 70% attention map alignment (MSE)  
   - 30% class probability matching (KL divergence)  

**Training Schedule**:  
| Phase       | Temperature (T) | Epochs |  
|-------------|-----------------|--------|  
| Warmup      | 3.0 â†’ 2.0       | 1-20   |  
| Fine-tuning | 1.5 â†’ 1.0       | 21-50  |  

---

### Day 5: Visualization & Debugging  
**Metrics to Plot**:  
1. Layer-wise attention weights across training epochs.  
2. Attention map overlays comparing teacher/student focus regions.  

**Success Criteria**:  
- Student attention patterns align visually with teacherâ€™s spatial focus.  

---

### Day 6: Quantitative Evaluation  
**Expected Results**:  
| Model              | Accuracy | Parameters |  
|--------------------|----------|------------|  
| Teacher (ViT-Tiny) | 82.1%    | 5.7M       |  
| Student (CNN)      | 80.9%    | 1.8M       |  
| Baseline CNN       | 78.3%    | 1.2M       |  

---

### Day 7: Analysis & Reporting  
**Key Insights**:  
1. Middle ViT layers (4-8) contribute 65-70% of total attention weight.  
2. Dynamic weighting improves accuracy by +1.2% vs uniform layer treatment.  

**Extensions Proposed**:  
- Cross-modal distillation (ViT â†’ Audio CNN)  
- Attention-guided pruning for student model compression  

---

## ðŸš€ Running the Project  
1. **Teacher Training**:  
python train_teacher.py --epochs 30

text

2. **Distillation**:  
python train_distill.py --alpha 0.7 --temp 3.0

text

---

## ðŸ“š References  
1. [Vision Transformers in `timm`](https://huggingface.co/docs/timm/index)  
2. [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)  
3. [Knowledge Distillation Review](https://arxiv.org/abs/2006.05525)  
